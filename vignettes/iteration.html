<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Ioannis Kosmidis" />

<meta name="date" content="2017-03-21" />

<title>Bias reduction in generalized linear models</title>






<link href="data:text/css;charset=utf-8,body%20%7B%0Abackground%2Dcolor%3A%20%23fff%3B%0Amargin%3A%201em%20auto%3B%0Amax%2Dwidth%3A%20700px%3B%0Aoverflow%3A%20visible%3B%0Apadding%2Dleft%3A%202em%3B%0Apadding%2Dright%3A%202em%3B%0Afont%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0Afont%2Dsize%3A%2014px%3B%0Aline%2Dheight%3A%201%2E35%3B%0A%7D%0A%23header%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0A%23TOC%20%7B%0Aclear%3A%20both%3B%0Amargin%3A%200%200%2010px%2010px%3B%0Apadding%3A%204px%3B%0Awidth%3A%20400px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Aborder%2Dradius%3A%205px%3B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Afont%2Dsize%3A%2013px%3B%0Aline%2Dheight%3A%201%2E3%3B%0A%7D%0A%23TOC%20%2Etoctitle%20%7B%0Afont%2Dweight%3A%20bold%3B%0Afont%2Dsize%3A%2015px%3B%0Amargin%2Dleft%3A%205px%3B%0A%7D%0A%23TOC%20ul%20%7B%0Apadding%2Dleft%3A%2040px%3B%0Amargin%2Dleft%3A%20%2D1%2E5em%3B%0Amargin%2Dtop%3A%205px%3B%0Amargin%2Dbottom%3A%205px%3B%0A%7D%0A%23TOC%20ul%20ul%20%7B%0Amargin%2Dleft%3A%20%2D2em%3B%0A%7D%0A%23TOC%20li%20%7B%0Aline%2Dheight%3A%2016px%3B%0A%7D%0Atable%20%7B%0Amargin%3A%201em%20auto%3B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dcolor%3A%20%23DDDDDD%3B%0Aborder%2Dstyle%3A%20outset%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0Aborder%2Dwidth%3A%202px%3B%0Apadding%3A%205px%3B%0Aborder%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dstyle%3A%20inset%3B%0Aline%2Dheight%3A%2018px%3B%0Apadding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0Aborder%2Dleft%2Dstyle%3A%20none%3B%0Aborder%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Ap%20%7B%0Amargin%3A%200%2E5em%200%3B%0A%7D%0Ablockquote%20%7B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Apadding%3A%200%2E25em%200%2E75em%3B%0A%7D%0Ahr%20%7B%0Aborder%2Dstyle%3A%20solid%3B%0Aborder%3A%20none%3B%0Aborder%2Dtop%3A%201px%20solid%20%23777%3B%0Amargin%3A%2028px%200%3B%0A%7D%0Adl%20%7B%0Amargin%2Dleft%3A%200%3B%0A%7D%0Adl%20dd%20%7B%0Amargin%2Dbottom%3A%2013px%3B%0Amargin%2Dleft%3A%2013px%3B%0A%7D%0Adl%20dt%20%7B%0Afont%2Dweight%3A%20bold%3B%0A%7D%0Aul%20%7B%0Amargin%2Dtop%3A%200%3B%0A%7D%0Aul%20li%20%7B%0Alist%2Dstyle%3A%20circle%20outside%3B%0A%7D%0Aul%20ul%20%7B%0Amargin%2Dbottom%3A%200%3B%0A%7D%0Apre%2C%20code%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0Aborder%2Dradius%3A%203px%3B%0Acolor%3A%20%23333%3B%0Awhite%2Dspace%3A%20pre%2Dwrap%3B%20%0A%7D%0Apre%20%7B%0Aborder%2Dradius%3A%203px%3B%0Amargin%3A%205px%200px%2010px%200px%3B%0Apadding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Acode%20%7B%0Afont%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0Afont%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0Apadding%3A%202px%200px%3B%0A%7D%0Adiv%2Efigure%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF%3B%0Apadding%3A%202px%3B%0Aborder%3A%201px%20solid%20%23DDDDDD%3B%0Aborder%2Dradius%3A%203px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Amargin%3A%200%205px%3B%0A%7D%0Ah1%20%7B%0Amargin%2Dtop%3A%200%3B%0Afont%2Dsize%3A%2035px%3B%0Aline%2Dheight%3A%2040px%3B%0A%7D%0Ah2%20%7B%0Aborder%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Apadding%2Dbottom%3A%202px%3B%0Afont%2Dsize%3A%20145%25%3B%0A%7D%0Ah3%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Afont%2Dsize%3A%20120%25%3B%0A%7D%0Ah4%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0Amargin%2Dleft%3A%208px%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Ah5%2C%20h6%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ccc%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Aa%20%7B%0Acolor%3A%20%230033dd%3B%0Atext%2Ddecoration%3A%20none%3B%0A%7D%0Aa%3Ahover%20%7B%0Acolor%3A%20%236666ff%3B%20%7D%0Aa%3Avisited%20%7B%0Acolor%3A%20%23800080%3B%20%7D%0Aa%3Avisited%3Ahover%20%7B%0Acolor%3A%20%23BB00BB%3B%20%7D%0Aa%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0Aa%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%20code%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%0A" rel="stylesheet" type="text/css" />

</head>

<body>




<h1 class="title toc-ignore">Bias reduction in generalized linear models</h1>
<h4 class="author"><em><a href="http://www.ucl.ac.uk/~ucakiko/">Ioannis Kosmidis</a></em></h4>
<h4 class="date"><em>21 March 2017</em></h4>



<div id="the-brglm2-package" class="section level1">
<h1>The <strong>brglm2</strong> package</h1>
<p><a href="https://github.com/ikosmidis/brglm2"><strong>brglm2</strong></a> provides tools for the estimation and inference from <a href="https://en.wikipedia.org/wiki/Generalized_linear_model">generalized linear models</a> using various methods for bias reduction <span class="citation">(Kosmidis 2014)</span>. Reduction of estimation bias is achieved either through the adjusted score equations approach in <span class="citation">Firth (1993)</span> and <span class="citation">Kosmidis and Firth (2009)</span>, or through the direct subtraction of an estimate of the bias of the maximum likelihood estimator from the maximum likelihood estimates as prescribed in <span class="citation">Cordeiro and McCullagh (1991)</span>.</p>
<p>In the special case of generalized linear models for binomial and multinomial responses, the adjusted score equations approach returns estimates with improved frequentist properties, that are also always finite, even in cases where the maximum likelihood estimates are infinite, like in complete and quasi-complete separation as defined in <span class="citation">Albert and Anderson (1984)</span>.</p>
<p>The workhorse function is <a href="https://github.com/ikosmidis/brglm2/blob/master/R/brglmFit.R"><code>brglmFit</code></a>, which can be passed directly to the <code>method</code> argument of the <code>glm</code> function. <code>brglmFit</code> implements a quasi <a href="https://en.wikipedia.org/wiki/Scoring_algorithm">Fisher scoring</a> procedure, whose special cases result in various explicit and implicit bias reduction methods for generalized linear models <span class="citation">(the classification of bias reduction methods into explicit and implicit is given in Kosmidis 2014)</span>.</p>
</div>
<div id="this-vignette" class="section level1">
<h1>This vignette</h1>
<p>This vignette</p>
<ul>
<li>presents the bias-reducing adjustments to the score functions for generalized linear models</li>
<li>describes the fitting algorithm at the core of <strong>brglm2</strong></li>
</ul>
</div>
<div id="other-resources" class="section level1">
<h1>Other resources</h1>
<p>The bias-reducing quasi Fisher scoring iteration is also described in detail in the <a href="https://cran.r-project.org/package=enrichwith/vignettes/bias.html">bias vignette</a> of the <a href="https://cran.r-project.org/package=enrichwith"><strong>enrichwith</strong></a> R package. <span class="citation">Kosmidis and Firth (2010)</span> describe a parallel quasi <a href="https://en.wikipedia.org/wiki/Newton%27s_method">Newton-Raphson</a> procedure.</p>
<p>Most of the material in this vignette comes from a presentation by <a href="https://www.ucl.ac.uk/~ucakiko/">Ioannis Kosmidis</a> at the <a href="http://user2016.org">useR! 2016 international R User conference</a> at University of Stanford on 16 June 2016. The presentation was titled “Reduced-bias inference in generalized linear models” and can be watched online at this <a href="https://channel9.msdn.com/Events/useR-international-R-User-conference/useR2016/brglm-Reduced-bias-inference-in-generalized-linear-models">link</a>.</p>
</div>
<div id="generalized-linear-models" class="section level1">
<h1>Generalized linear models</h1>
<div id="model" class="section level3">
<h3>Model</h3>
Suppose that <span class="math inline">\(y_1, \ldots, y_n\)</span> are observations on independent random variables <span class="math inline">\(Y_1, \ldots, Y_n\)</span>, each with probability density/mass function of the form <span class="math display">\[
f_{Y_i}(y) = \exp\left\{\frac{y \theta_i - b(\theta_i) - c_1(y)}{\phi/m_i} - \frac{1}{2}a\left(-\frac{m_i}{\phi}\right) + c_2(y) \right\}
\]</span> for some sufficiently smooth functions <span class="math inline">\(b(.)\)</span>, <span class="math inline">\(c_1(.)\)</span>, <span class="math inline">\(a(.)\)</span> and <span class="math inline">\(c_2(.)\)</span>, and fixed observation weights <span class="math inline">\(m_1, \ldots, m_n\)</span>. The expected value and the variance of <span class="math inline">\(Y_i\)</span> are then
\begin{align*}
      E(Y_i) &amp; = \mu_i =  b'(\theta_i) \\
      Var(Y_i) &amp; = \frac{\phi}{m_i}b''(\theta_i) = \frac{\phi}{m_i}V(\mu_i)
\end{align*}
<p>Hence, in this parameterization, <span class="math inline">\(\phi\)</span> is a dispersion parameter.</p>
<p>A generalized linear model links the mean <span class="math inline">\(\mu_i\)</span> to a linear predictor <span class="math inline">\(\eta_i\)</span> as <span class="math display">\[
g(\mu_i) = \eta_i = \sum_{t=1}^p \beta_t x_{it}
\]</span> where <span class="math inline">\(g(.)\)</span> is a monotone, sufficiently smooth link function, taking values on <span class="math inline">\(\Re\)</span>, <span class="math inline">\(x_{it}\)</span> is the <span class="math inline">\((i,t)th\)</span> component of a model matrix <span class="math inline">\(X\)</span>, and <span class="math inline">\(\beta = (\beta_1, \ldots, \beta_p)^\top\)</span>.</p>
</div>
<div id="score-functions-and-information-matrix" class="section level3">
<h3>Score functions and information matrix</h3>
The derivatives of the log-likelihood about <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\phi\)</span> (score functions) are
\begin{align*}
    s_\beta(\beta, \phi) &amp; = \frac{1}{\phi}X^TWD^{-1}(y - \mu) \\
    s_\phi(\beta, \phi) &amp; = \frac{1}{2\phi^2}\sum_{i = 1}^n (q_i - \rho_i)
\end{align*}
<p>with <span class="math inline">\(y = (y_1, \ldots, y_n)^\top\)</span>, <span class="math inline">\(\mu = (\mu_1, \ldots, \mu_n)^\top\)</span>, <span class="math inline">\(W = {\rm diag}\left\{w_1, \ldots, w_n\right\}\)</span> and <span class="math inline">\(D = {\rm diag}\left\{d_1, \ldots, d_n\right\}\)</span>, where <span class="math inline">\(w_i = m_i d_i^2/V(\mu_i)\)</span> is the <span class="math inline">\(i\)</span>th working weight, and <span class="math inline">\(d_i = d\mu_i/d\eta_i\)</span>. Furthermore, <span class="math inline">\(q_i = -2 m_i \{y_i\theta_i - b(\theta_i) - c_1(y_i)\}\)</span> and <span class="math inline">\(\rho_i = m_i a'(-m_i/\phi)\)</span> are the <span class="math inline">\(i\)</span>th deviance residual (e.g. as implemented in the <code>dev.resid</code> component of a <code>family</code> object) and its expectation, respectively.</p>
<p>The expected information matrix about <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\phi\)</span> is <span class="math display">\[
i(\beta, \phi) =
\left[
\begin{array}{cc}
i_{\beta\beta}(\beta, \phi) &amp; 0_p \\
0_p^\top &amp; i_{\phi\phi}(\beta, \phi)
\end{array}
\right]
=
\left[
\begin{array}{cc}
\frac{1}{\phi} X^\top W X &amp; 0_p \\
0_p^\top &amp; \frac{1}{2\phi^4}\sum_{i = 1}^n m_i^2 a''(-m_i/\phi)
\end{array}
\right]\,,
\]</span> where <span class="math inline">\(0_p\)</span> is a <span class="math inline">\(p\)</span>-vector of zeros.</p>
</div>
<div id="maximum-likelihood-estimation" class="section level3">
<h3>Maximum likelihood estimation</h3>
<p>The maximum likelihood estimator of <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\phi\)</span> satisfies <span class="math inline">\(s_\beta(\hat\beta,\hat\phi) = 0_p\)</span> and <span class="math inline">\(s_\phi(\hat\beta, \hat\phi) = 0\)</span>.</p>
</div>
<div id="bias-reducing-adjusted-score-functions" class="section level3">
<h3>Bias-reducing adjusted score functions</h3>
Let <span class="math inline">\(A_\beta(\beta, \phi) = -i_\beta(\beta, \phi) b_\beta(\beta, \phi)\)</span> and <span class="math inline">\(A_\phi(\beta, \phi) = -i_\phi(\beta, \phi) b_\phi(\beta, \phi)\)</span>, where <span class="math inline">\(b_\beta(\beta, \phi)\)</span> and <span class="math inline">\(b_\phi(\beta, \phi)\)</span> are the first terms in the expansion of the bias of the maximum likelihood estimator of the regression parameters <span class="math inline">\(\beta\)</span> and dispersion <span class="math inline">\(\phi\)</span>, respectively. The results in <span class="citation">Firth (1993)</span> can be used to show that the solution of the adjusted score equations
\begin{align*}
s_\beta(\beta,\phi) + A_\beta(\beta, \phi) &amp; = 0_p \\
s_\phi(\beta, \phi) + A_\phi(\beta, \phi) &amp; = 0
\end{align*}
<p>results in estimators <span class="math inline">\(\tilde\beta\)</span> and <span class="math inline">\(\tilde\phi\)</span> with bias of smaller asymptotic order than the maximum likelihood estimator.</p>
The results in either <span class="citation">Kosmidis and Firth (2009)</span> or <span class="citation">Cordeiro and McCullagh (1991)</span> can then be used to re-express the adjustments in forms that are convenient for implementation. In particular, and after some algebra the bias-reducing adjustments for generalized linear models are
\begin{align*}
 A_\beta(\beta, \phi) &amp; = X^\top W \xi \,, \\
 A_\phi(\beta, \phi) &amp; = \frac{(p - 2)\phi\sum_{i = 1}^n m_i^2
                   a''(-m_i/\phi) + \sum_{i = 1}^n m_i^3
                   a'''(-m_i/\phi))}{2\phi^2\sum_{i = 1}^n m_i^2
                   a''(-m_i/\phi)}
\end{align*}
<p>where <span class="math inline">\(\xi = (\xi_1, \ldots, \xi_n)^T\)</span> with <span class="math inline">\(\xi_i = h_id_i'/(2d_iw_i)\)</span>, <span class="math inline">\(d_i' = d^2\mu_i/d\eta_i^2\)</span> and <span class="math inline">\(h_i\)</span> is the “hat” value for the <span class="math inline">\(i\)</span>th observation (see, e.g. <code>?hatvalues</code>).</p>
</div>
</div>
<div id="fitting-algorithm-in-brglmfit" class="section level1">
<h1>Fitting algorithm in <code>brglmFit</code></h1>
<p><code>brglmFit</code> implements a quasi Fisher scoring procedure for solving the adjusted score equations <span class="math inline">\(s_\beta(\beta,\phi) + A_\beta(\beta, \phi) = 0_p\)</span> and <span class="math inline">\(s_\phi(\beta, \phi) + A_\phi(\beta, \phi) = 0\)</span>. The iteration consists of an outer loop and an inner loop that implements step-halving. The algorithm is as follows:</p>
<div id="input" class="section level3">
<h3>Input</h3>
<ul>
<li><span class="math inline">\(s_\beta(\beta, \phi)\)</span>, <span class="math inline">\(i_{\beta\beta}(\beta, \phi)\)</span>, <span class="math inline">\(A_\beta(\beta, \phi)\)</span></li>
<li><span class="math inline">\(s_\phi(\beta, \phi)\)</span>, <span class="math inline">\(i_{\phi\phi}(\beta, \phi)\)</span>, <span class="math inline">\(A_\phi(\beta, \phi)\)</span></li>
<li>Starting values <span class="math inline">\(\beta^{(0)}\)</span> and <span class="math inline">\(\phi^{(0)}\)</span></li>
<li><span class="math inline">\(\epsilon &gt; 0\)</span>: tolerance for the <span class="math inline">\(L1\)</span> norm of the direction before reporting convergence</li>
<li><span class="math inline">\(M\)</span>: maximum number of halving steps that can be taken</li>
</ul>
</div>
<div id="output" class="section level3">
<h3>Output</h3>
<ul>
<li><span class="math inline">\(\tilde\beta\)</span>, <span class="math inline">\(\tilde\phi\)</span></li>
</ul>
</div>
<div id="iteration" class="section level3">
<h3>Iteration</h3>
<p><em>Initialize outer loop</em></p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(k \leftarrow 0\)</span></p></li>
<li><p><span class="math inline">\(\upsilon_\beta^{(0)} \leftarrow \left\{i_{\beta\beta}\left(\beta^{(0)}, \phi^{(0)}\right)\right\}^{-1} \left\{s_\beta\left(\beta^{(0)}, \phi^{(0)}\right) + A_\beta\left(\beta^{(0)}, \phi^{(0)}\right)\right\}\)</span></p></li>
<li><p><span class="math inline">\(\upsilon_\phi^{(0)} \leftarrow \left\{i_{\phi\phi}\left(\beta^{(0)}, \phi^{(0)}\right)\right\}^{-1} \left\{s_\phi\left(\beta^{(0)}, \phi^{(0)}\right) + A_\phi\left(\beta^{(0)}, \phi^{(0)}\right)\right\}\)</span></p></li>
</ol>
<p><em>Initialize inner loop</em></p>
<ol start="4" style="list-style-type: decimal">
<li><p><span class="math inline">\(m \leftarrow 0\)</span></p></li>
<li><p><span class="math inline">\(b^{(m)} \leftarrow \beta^{(k)}\)</span></p></li>
<li><p><span class="math inline">\(f^{(m)} \leftarrow \phi^{(k)}\)</span></p></li>
<li><p><span class="math inline">\(v_\beta^{(m)} \leftarrow \upsilon_\beta^{(k)}\)</span></p></li>
<li><p><span class="math inline">\(v_\phi^{(m)} \leftarrow \upsilon_\phi^{(k)}\)</span></p></li>
<li><p><span class="math inline">\(d \leftarrow \left|v_\beta^{(m)}\right|_1 + \left|v_\phi^{(m)}\right|\)</span></p></li>
</ol>
<p><em>Update parameters</em></p>
<ol start="10" style="list-style-type: decimal">
<li><p><span class="math inline">\(b^{(m + 1)} \leftarrow b^{(m)} + 2^{-m} v_\beta^{(m)}\)</span></p></li>
<li><p><span class="math inline">\(f^{(m + 1)} \leftarrow f^{(m)} + 2^{-m} v_\phi^{(m)}\)</span></p></li>
</ol>
<p><em>Update direction</em></p>
<ol start="12" style="list-style-type: decimal">
<li><p><span class="math inline">\(v_\beta^{(m + 1)} \leftarrow \left\{i_{\beta\beta}\left(b^{(m + 1)}, f^{(m + 1)}\right)\right\}^{-1} \left\{s_\beta\left(b^{(m + 1)}, f^{(m + 1)}\right) + A_\beta\left(b^{(m + 1)}, f^{(m + 1)}\right)\right\}\)</span></p></li>
<li><p><span class="math inline">\(v_\phi^{(m + 1)} \leftarrow \left\{i_{\phi\phi}\left(b^{(m + 1)}, f^{(m + 1)}\right)\right\}^{-1} \left\{s_\phi\left(b^{(m + 1)}, f^{(m + 1)}\right) + A_\phi\left(b^{(m + 1)}, f^{(m + 1)}\right)\right\}\)</span></p></li>
</ol>
<p><em>Continue or break halving within inner loop</em></p>
<ol start="14" style="list-style-type: decimal">
<li><p>if <span class="math inline">\(m + 1 &lt; M\)</span> and <span class="math inline">\(\left|v_\beta^{(m + 1)}\right|_1 + \left|v_\phi^{(m + 1)}\right| &gt; d\)</span></p>
<p>14.1. <span class="math inline">\(m \leftarrow m + 1\)</span></p>
<p>14.2. GO TO 10</p></li>
<li><p>else</p>
<p>15.1. <span class="math inline">\(\beta^{(k + 1)} \leftarrow b^{(m + 1)}\)</span></p>
<p>15.2. <span class="math inline">\(\phi^{(k + 1)} \leftarrow f^{(m + 1)}\)</span></p>
<p>15.3. <span class="math inline">\(\upsilon_\beta^{(k + 1)} \leftarrow v_b^{(m + 1)}\)</span></p>
<p>15.4. <span class="math inline">\(\upsilon_\phi^{(k + 1)} \leftarrow v_f^{(m + 1)}\)</span></p></li>
</ol>
<p><em>Continue or break outer loop</em></p>
<ol start="16" style="list-style-type: decimal">
<li><p>if <span class="math inline">\(k + 1 &lt; K\)</span> and <span class="math inline">\(\left|\upsilon_\beta^{(k + 1)}\right|_1 + \left|\upsilon_\phi^{(k + 1)}\right| &gt; \epsilon\)</span></p>
<p>16.1 <span class="math inline">\(k \leftarrow k + 1\)</span></p>
<p>16.2. GO TO 4</p></li>
<li><p>else</p>
<p>17.1. <span class="math inline">\(\tilde\beta \leftarrow \beta^{(k + 1)}\)</span></p>
<p>17.2. <span class="math inline">\(\tilde\phi \leftarrow \phi^{(k + 1)}\)</span></p>
<p>17.3. STOP</p></li>
</ol>
</div>
</div>
<div id="notes" class="section level1">
<h1>Notes</h1>
<ul>
<li><p>For <span class="math inline">\(K = M = 1\)</span>, <span class="math inline">\(\beta^{(0)} = \hat\beta\)</span> and <span class="math inline">\(\phi^{(0)} = \hat\phi\)</span>, the above iteration computes the bias-corrected estimates proposed in <span class="citation">Cordeiro and McCullagh (1991)</span>.</p></li>
<li><p>The steps where <span class="math inline">\(\phi\)</span> and the <span class="math inline">\(\phi\)</span> direction are updated are ignored for generalized linear models with known dispersion parameter, like in models with binomial and poisson responses. Also, in that case, <span class="math inline">\(v_\phi^{(.)}\)</span> and <span class="math inline">\(\upsilon_\phi^{(.)}\)</span> in steps 9, 14 and 16 are set to zero.</p></li>
<li><p>The implementation of the adjusted score functions requires ready implementations of <span class="math inline">\(d^2\mu_i/d\eta_i^2\)</span>, <span class="math inline">\(a'(.)\)</span>, <span class="math inline">\(a''(.)\)</span> and <span class="math inline">\(a'''(.)\)</span>. The <a href="https://cran.r-project.org/package=enrichwith"><strong>enrichwith</strong></a> R package is used internally to enrich the base <code>family</code> and <code>link-glm</code> objects with implementations of those functions (see <code>?enrich.family</code> and <code>?enrich.link-glm</code>).</p></li>
<li><p>The above iteration can be used to implement a variety of additive adjustments to the score function, by supplying the algorithm with appropriate adjustment functions <span class="math inline">\(A_\beta(\beta, \phi)\)</span> and <span class="math inline">\(A_\phi(\beta, \phi)\)</span></p></li>
</ul>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-albert:84">
<p>Albert, A., and J. A. Anderson. 1984. “On the Existence of Maximum Likelihood Estimates in Logistic Regression Models.” <em>Biometrika</em> 71 (1): 1–10.</p>
</div>
<div id="ref-cordeiro:91">
<p>Cordeiro, G. M., and P. McCullagh. 1991. “Bias Correction in Generalized Linear Models.” <em>Journal of the Royal Statistical Society, Series B: Methodological</em> 53 (3): 629–43.</p>
</div>
<div id="ref-firth:93">
<p>Firth, D. 1993. “Bias Reduction of Maximum Likelihood Estimates.” <em>Biometrika</em> 80 (1): 27–38.</p>
</div>
<div id="ref-kosmidis:14">
<p>Kosmidis, I. 2014. “Bias in Parametric Estimation: Reduction and Useful Side-Effects.” <em>Wiley Interdisciplinary Reviews: Computational Statistics</em> 6 (3). John Wiley &amp; Sons, Inc.: 185–96. doi:<a href="https://doi.org/10.1002/wics.1296">10.1002/wics.1296</a>.</p>
</div>
<div id="ref-kosmidis:09">
<p>Kosmidis, I., and D. Firth. 2009. “Bias Reduction in Exponential Family Nonlinear Models.” <em>Biometrika</em> 96 (4): 793–804. doi:<a href="https://doi.org/10.1093/biomet/asp055">10.1093/biomet/asp055</a>.</p>
</div>
<div id="ref-kosmidis:10">
<p>———. 2010. “A Generic Algorithm for Reducing Bias in Parametric Estimation.” <em>Electronic Journal of Statistics</em> 4: 1097–1112. doi:<a href="https://doi.org/10.1214/10-EJS579">10.1214/10-EJS579</a>.</p>
</div>
</div>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
